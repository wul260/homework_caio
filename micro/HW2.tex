\documentclass{article}
\usepackage{graphicx}
\usepackage{amsmath}
\input{structure.tex} % Include the file specifying the document structure and custom commands

%----------------------------------------------------------------------------------------
%	ASSIGNMENT INFORMATION
%----------------------------------------------------------------------------------------

\title{Game Theory: Homework} % Title of the assignment

\author{Caio Figueiredo} % Author name and email address

%----------------------------------------------------------------------------------------

\begin{document}

\maketitle % Print the title

%	PROBLEM 1 {{{
%----------------------------------------------------------------------------------------

\section{Question 1}
\subsection{(a)} % {{{2

We can rewrite $V(\mu)$ as:

\[ 
  V(\mu) = (1 - \delta)V^*(\mu) 
\]
\begin{equation}
  \begin{split}
    V^*(\mu) & = \max_{a \in \{0, 1\}}\{(\mathbb{E}(y_t) - \frac{1}{2})a + \delta \mathbb{E}(V^*(\mu'))\} \\
             & = \max_{a \in \{0, 1\}}\{(\frac{1}{2}\mu  - \frac{1}{4})a + \delta \mathbb{E}(V^*(\mu'))\}
  \end{split}
\end{equation}

where:
\[
  \mu' =  \mu \texttt{ if } a = 0
\]
\[
  \mu' = \frac{\frac{3}{4}\mu}{\frac{1}{4} + \frac{1}{2}\mu} \texttt{ if } a = 1, y = 1
\]
\[
  \mu' = \frac{\frac{1}{4}\mu}{\frac{3}{4} - \frac{1}{2}\mu} \texttt{ if } a = 1, y = 0
\]

Now we can use the standard Recursive Dynamic proof, define:
\[
  T(f)(x) = \max_{a \in \{0, 1\}}\{(\frac{1}{2}\mu  - \frac{1}{4})a + \delta f(\mu') \}
\]

$T$ maps weakly increasing functions in weakly increasing functions because $\mu'$
is stricly increasing in $\mu$ and $(\frac{1}{2}\mu  - \frac{1}{4})a$ is stricly
increasing as long as $a = 1$ and constant as long as $a = 0$.

Therefore by the Contraction Mapping Theorem the fix point of $T$, which is $V^*$,
is weakly increasing. Moreover, since $V^*$ is weakly increasing and $(1 - \delta) > 0$,
then $V$ is weakly increasing.
%2}}}

\subsection{(b)} %{{{2

First notice that we can use the same argument of (a) to show that $V^*$ is continuous.

Now notice that for $\mu = 0$, $\mu' = 0\ \forall a,y$ and for $\mu = 1$,
$\mu' = 1 \forall a,y$, therefore:

\[
  \begin{split}
    V^*(0) & =  \max_{a \in \{0, 1\}}\{- \frac{1}{4}a + \delta V^*(0) \} \\
           & = \frac{\max_{a \in \{0, 1\}}\{- \frac{1}{4}a\}}{(1 - \delta)} \\
           & = 0 \\
    V(0)   & = 0
  \end{split}
  \ \    
  \begin{split}
    V^*(1) & =  \max_{a \in \{0, 1\}}\{\frac{1}{4}a + \delta V^*(1) \} \\
           & = \frac{\max_{a \in \{0, 1\}}\{\frac{1}{4}a\}}{(1 - \delta)} \\
           & = \frac{1}{4(1 - \delta)} > 0 \\
    V(1)   & = \frac{1}{4}
  \end{split}
\]

Since $V$ is weakly increasing that must be $\mu^* \ge 0$ such that for for all 
$0 \ge \mu < \mu^*, V(\mu) = 0$, and for all $1 \ge \mu > \mu, V(\mu) > 0$.

Moreover, $0 \le \mu < \mu^*$, optimal $a$ is $0$ and for $\mu^* < \mu \le 1$, 
optimal $a$ is $1$. Since $V^*$ is continuous at $\mu^*$ the agent is indifferent.
%2}}}

\subsection{(c)} %{{{2

First notice that the statement is not true for $\mu = 1$, in this case the agent
never update his belief and always choose $a = 1$.

However, for any other $\mu$, such that $\mu^* < \mu < 1$, there exists finite number of bad signals
in sequence that is enough to bring it down to a number below $\mu^*$. To see that define:
$\mu_0^n$ the belief after observing $y = 0, n$ times in a row. It easy to show that:

\[
  \mu_0^n = \frac{\mu}{3^n - \sum_{i = 0}^n(3^{i-1}2)\mu}
\]

It is also simple to show that $\mu_0^n \to 0$ as $n \to \infty$. Therefore:
we have for any $\epsilon > 0$, in particular, for $\epsilon = \mu^*, \exists
n^* \in \mathcal{N}$ such that for any $n > n^*, \mu_0^n < \mu^*$.

We already stablish that one below $\mu^*$ the agent always choose $a = 0$, which implies
he don't update his belief anymore and therefore always choose $a = 0$. Since $n^*$ is a
finite number the probability of observing the bad signal $n^*$ times is positive and this
part of the statement is proven.

We are left to prove that there is a probability that the belief never falls below $\mu^*$.
%I first stablish some facts:


%\begin{itemize}
  %\item $\mu_1^n \to 1$, that is the belief after observing the good signal $n$ times
    %in a row converges to $1$. This can be done in a similar way as above.
  %\item Updating after a good signal reverses a bad signal update and vice versa,
    %that is:
%\end{itemize}

%\[
  %\mu_1(\mu_0(\mu)) = \mu_0(\mu_1(\mu)) = \mu
%\]

%Proof:

%\[ 
%\begin{split}
  %\mu_1(\mu_0(\mu)) & = \frac{3\frac{\mu}{3 - 2\mu}}{1 + 2\frac{\mu}{3 - 2\mu}} \\
                    %& = \frac{3\mu}{3 - 2\mu + 2\mu} = \mu \\
  %\mu_0(\mu_1(\mu)) & = \frac{\frac{3\mu}{1 + 2\mu}}{3 - 2\frac{3\mu}{1 + 2\mu}} \\
                    %& = \frac{3\mu}{3 - 6\mu + 6\mu} = \mu
%\end{split}
%\]

I now define a new state:

\[
  s_t = (\texttt{\# Good Signals}) - (\texttt{\# Bad Signals})
\]

%It's clear from the reversibility noted above that $s_t > 0$ means that the
%belief at $t$ is greater than the prior.

\newcommand{\Pn}{P_{\infty}}
Also define $\Pn(s)$ as the probability that at state $s$ the agent
only plays $a = 1$ forever. Notice that by the same argument as above, as
$s \to \infty$ the belief goes to $1$ and therefore $\Pn(\infty) = 1$.
We have:

\[
  \Pn(s) = P(y=1)\Pn(s+1) + P(y=0)\Pn(s-1)
\]

Which follows from the definition of $\Pn$. Moreover, since $\theta = 1,
P(y=1) = \frac{3}{4}$ and $P(y=0) = \frac{1}{4}$. For simplification we
further assume that the prior $\mu$ is such that at $s = 0$ one bad return
is enough to bring our belief below $\mu^*$, which implies that $\Pn(-1) = 0$.
This is WLOG because if the affirmation hold for a $\mu$ this low is must also
holds for higher $\mu$s
Therefore:

\[
\begin{split}
  \Pn(0)            & = \frac{3}{4}\Pn(1) \\
                    & = \frac{3}{4}(\frac{3}{4}\Pn(2) + \frac{1}{4}\Pn(0)) \\
  \frac{13}{16}\Pn(0)& = \frac{13}{9}\Pn(2) \\
  \text{Therefore:} & \\
  \Pn(1)            & = \frac{4}{3}\Pn(0)\\
  \Pn(2)            & = \frac{13}{9}\Pn(0)\\
\end{split}
\]

Moreover:

\[
\begin{split}
  \Pn(n)            & = \frac{3}{4}\Pn(n + 1) + \frac{1}{4}\Pn(n - 1) \\
  \frac{3}{4}\Pn(n+1)& = \Pn(n) - \frac{1}{4}\Pn(n-1) \\
  \Pn(n+1)          & = \frac{4}{3}\Pn(n) - \frac{1}{3}\Pn(n - 1) \\
  \Pn(n+1) - \frac{1}{3}\Pn(n) & = \Pn(n) - \frac{1}{3}\Pn(n - 1) \\
                    & = \Pn(2) - \frac{1}{3}\Pn(1) \\
                    & = \frac{13}{9}\Pn(0) - \frac{1}{3}\frac{3}{4}\Pn(0)) \\
  \Pn(0)            & = \Pn(n+1) - \frac{1}{3}\Pn(n) \\
\end{split}
\]
\[
\begin{split}
  \text{As $n \to \infty$:} & \\
  \Pn(0)            & = \frac{2}{3}\Pn(\infty) \\
  \Pn(0)            & = \frac{2}{3}
\end{split}
\]

Therefore the probability of playing $a = 1$ forever when $s = 0$, that is
the initial state, is $ \frac{2}{3}$ which concludes our proof.
%2}}}

\subsection{(d)} % {{{2

We can use the very same technique as before. Redefine $\Pn(s)$ as the probability
that the agent eventually takes action $0$ forever. Remember, $\Pn(-\infty) = 1$.

\[
\begin{split}
  \Pn(n)            & = \frac{1}{4}\Pn(n + 1) + \frac{3}{4}\Pn(n - 1) \\
  \Pn(n + 1)        & = 4\Pn(n) - 3\Pn(n-1) \\
  \Pn(n + 1)-\Pn(n) & = 3(\Pn(n) - \Pn(n-1)) \\
  \Pn(n + 2)-\Pn(n+1)&= 3^2(\Pn(n) - \Pn(n-1)) \\
  \Pn(\infty)-\Pn(\infty)&= 3^\infty(\Pn(n) - \Pn(n-1)) \\
  0                 & = \Pn(n) - \Pn(n-1) \\
  \Pn(n)            & = \Pn(n-1) \\
  \Pn(0)            & = \Pn(-1) = \ldots = \Pn(-\infty) = 1 \\
\end{split}
\]
%2}}}

% }}}

%	PROBLEM 2 {{{
%----------------------------------------------------------------------------------------
\section{Question 2}
\label{sec:Question 2}

\newcommand{\sumt}{\sum_{t=0}^\infty}
\newcommand{\sumi}{\sum_{i=1}^\infty}
\newcommand{\Pth}{P_{\theta^*}(y_i|y_1, \ldots, y_t)}
\newcommand{\muth}{\mu(y_i|y_1, \ldots, y_t)}
\[
  \begin{split}
  |V^c(\delta, \theta) - V(\delta, \theta)| & = \left|
      \sumt \delta^t a^c_t \left(\sumi y_i \Pth\right) - 
          \sumt \delta^t a_t \left(\sumi y_i \Pth\right)
          \right| \\
    & \le \left|
        \sumt \delta^t a^c_t \left(\sumi y_i \Pth\right) - 
            \sumt \delta^t a_t \left(\sumi y_i \muth\right)
            \right| + \\ 
      & \qquad\quad \left|
        \sumt \delta^t a_t \left(\sumi y_i \Pth\right) - 
            \sumt \delta^t a_t \left(\sumi y_i \muth\right)
            \right| \\
    & \le 2\left(
        \sumt \delta^{2t} \max_{y \in Y} y^2
            \left(\sumi |\Pth \muth|\right)^2\right)^{1/2} + \\
    & \le 2\max_{y \in Y} |y|^2 \left(\sumt \delta^{2t}\right)^{1/2} 
        \left(\sumt \sup_{y \in Y}\{
            P_{\theta^*}(y|y_1, \ldots, y_t)
            \mu(y|y_1, \ldots, y_t)\}^2 |y|\right)^{1/2} \\
    & \le \frac{2|y|^{1/2}\max_{y \in Y} |y|^2}{(1 - \delta^2)^{1/2}} 
          \left(\sumt \frac{1}{2}KL(\theta^*|\mu)\right)^{1/2} \\
    & \le \frac{2|y|^{1/2}\max_{y \in Y} |y|^2}{(1 - \delta^2)^{1/2}} 
          \left(\sumt -\ln e^{-(1 -\delta)^{2(t+1)}}\right)^{1/2} \\
    & = \frac{2|y|^{1/2}\max_{y \in Y} |y|^2}{(1 - \delta^2)^{1/2}} 
          \frac{1 -\delta}{(1 - (1 - \delta)^2)^{1/2}} \\
    & = 2|y|^{1/2}\max_{y \in Y} |y|^2
          \left(\frac{1 -\delta}{(1 + \delta)(1 - (1 - \delta)^2)}\right)^{1/2} \\
  \end{split}
\]

Finally, notice that $1 - \delta \to 0$ as $\delta \to 1$ while
$(1 + \delta)(1 - (1 - \delta)^2) \to 2$. Therefore

\[
  \lim_{\delta \to 1} V^c(\delta, \theta) - V(\delta, \theta)) = 0
\]

%}}}

%	PROBLEM 3 {{{
%----------------------------------------------------------------------------------------

\section{Question 3}%
\label{sec:Question 3}

For simplification:

\[
\begin{split}
  P_{\gamma}(y) & = \mu P_{\gamma}(y| \theta = 1) + (1 - \mu)P_{\gamma}(y|\theta = -1) \\
  P_{\gamma}(1) & = \mu (1/2 + \gamma) + (1 - \mu)(1/2 \gamma) \\
                & = 1/2 + \gamma(2\mu - 1) \\
  P_{\gamma}(0) & = 1/2 - \gamma(2\mu - 1)
\end{split}
\]

Notice:

\[
  \lim_{\gamma \to 0} P_{\gamma}(0) = \lim_{\gamma \to 0} P_{\gamma}(1) = \frac{1}{2}
\]

The Baseyian updates are:

\[
  \mu_\gamma(1) = \frac{(1/2 + \gamma)\mu}{P_\gamma(1)}
\]
\[
  \mu_\gamma(0) = \frac{(1/2 - \gamma)\mu}{P_\gamma(0)}
\]

Therefore:

\[
  \lim_{\gamma \to 0} \mu_\gamma(0) = \lim_{\gamma \to 0} \mu_\gamma(1) = \mu
\]

and:

\[
  \lim_{\gamma \to 0} V(\mu_\gamma(0)) = \lim_{\gamma \to 0} V(\mu_\gamma(1)) = V(\mu)
\]

%We can now evaluate $V_(\mu_\gamma(0))$:

%\[
  %V_(\mu_\gamma(0)) & = 
    %\max_{a \in [0,1]} \frac{(1/2 - \gamma)\mu}{P_\gamma(0)}u(a, 1) +
      %\frac{P_\gamma(0) - (1/2 - \gamma)\mu}{P_\gamma(0)}u(a, -1) \\
                    %& = 
%\]

Finally:

\[
\begin{split}
  V_\gamma(\mu) & = P_\gamma(0)V(\mu_\gamma(0)) + P_\gamma(1)V(\mu_\gamma(1)) \\
                & = (1/2 + \gamma(2\mu -1))V(\mu_\gamma(1)) +
                    (1/2 - \gamma(2\mu -1))V(\mu_\gamma(0))
\end{split}
\]

and:

\[
  \lim_{\gamma \to 0} V_\gamma(\mu) = V(\mu)
\]

Whice make the question identity equals the derivative of $V_\gamma$ in relation
to $\gamma$:

\[
\frac{\partial V_\gamma(\mu)}{\partial \gamma} =
\lim_{\gamma \to 0} \frac{V_\gamma(\mu) - V(\mu)}{\gamma}
\]

We are left to prove that this derivative is zero at $\gamma$ = 0:

\[
\begin{split}
  \frac{\partial V_\gamma(\mu)}{\partial \gamma}(\gamma) & =
    (2\mu -1)V(\mu_\gamma(1)) +
    \frac{\partial V(\mu_\gamma(1))}{\partial \gamma}(1/2 + \gamma(2\mu - 1)) - \\
    & \qquad\quad
    (2\mu -1)V(\mu_\gamma(0)) +
    \frac{\partial V(\mu_\gamma(0))}{\partial \gamma}(1/2 - \gamma(2\mu - 1)) \\
  \frac{\partial V_\gamma(\mu)}{\partial \gamma}(0) & =
    (2\mu -1)V(\mu) +
    \frac{\partial V(\mu_0(1))}{\partial \gamma}(1/2) -
    (2\mu -1)V(\mu) +
    \frac{\partial V(\mu_0(0))}{\partial \gamma}(1/2) \\
  & =
    1/2 \left(\frac{\partial V(\mu_0(0))}{\partial \gamma} +
      \frac{\partial V(\mu_0(1))}{\partial \gamma}\right)
\end{split}
\]

But:

\[
\begin{split}
  V(\mu_\gamma(1)) & = \mu_\gamma u(a_1^*, 1) + (1 - \mu_\gamma) u(a_1^*, -1) \\
  & = P_\gamma(1)^{-1} 
    [(1/2 + \gamma)\mu u(a_1^*, 1) + \gamma(2\mu - 2)\mu u(a_1^*, -1)] \\
  \frac{\partial V(\mu_\gamma(1))}{\partial \gamma} & = P_\gamma(1)^{-1} 
    [\mu u(a_1^*) + (2\mu - 2)u(a_1^*, -1)] + \\
    & \qquad\quad
    \frac{1 - 2\mu}{(1/2 + \gamma(2\mu -1))^2} 
    [(1/2 + \gamma)\mu u(a_1^*,1) + \gamma(2\mu - 2) u(a_1^*, -1)] \\
  \frac{\partial V(\mu_0(1))}{\partial \gamma} & = 
    2[\mu u(a_1^*) + (2\mu - 2)u(a_1^*, -1)] +
    (4 - 8\mu)[1/2\mu u(a_1^*,1)]
\end{split}
\]

where $a_1^*$ is the argmax of $V$ for $\gamma = 0$ and:

\[
\begin{split}
  V(\mu_\gamma(0)) & =  P_\gamma(0)^{-1} 
    [(1/2 - \gamma)\mu u(a_0^*, 1) - \gamma(2\mu - 2)\mu u(a_0^*, -1)] \\
  \frac{\partial V(\mu_\gamma(0))}{\partial \gamma} & = P_\gamma(0)^{-1} 
    [-\mu u(a_0^*) - (2\mu - 2)u(a_0^*, -1)] + \\
    & \qquad\quad
    \frac{2\mu - 1}{(1/2 - \gamma(2\mu -1))^2}
    [(1/2 - \gamma)\mu u(a_0^*,1) - \gamma(2\mu - 2) u(a_0^*, -1)] \\
  \frac{\partial V(\mu_0(0))}{\partial \gamma} & = 
    2[-\mu u(a_0^*) - (2\mu - 2)u(a_0^*, -1)] +
    (8\mu - 4)[1/2\mu u(a_0^*,1)]
\end{split}
\]

and since $a_1^* \to a_0^*$ as $\gamma \to 0$, by what we have seem before,
we have:

\[
\begin{split}
  \frac{\partial V(\mu_0(0))}{\partial \gamma} +
    \frac{\partial V(\mu_0(1))}{\partial \gamma} & = 0 \\
  \frac{\partial V_\gamma(\mu)}{\partial \gamma}(0) & = 0 \\
  \lim_{\gamma \to 0} \frac{V_\gamma(\mu) - V(\mu)}{\gamma} & = 0
\end{split}
\]

$\hfill\blacksquare$ 

\subsection{b}

From (a) we can conclude:

%\[
%\begin{split}
  %\frac{\partial V_\gamma(\mu)}{\partial \gamma}(\gamma) & =
    %(2\mu -1)\left[
  %P_\gamma(1)^{-1} 
    %[(1/2 + \gamma)\mu u(a_1^*, 1) + \gamma(2\mu - 2)\mu u(a_1^*, -1)] \\
  %P_\gamma(0)^{-1} 
    %[(1/2 - \gamma)\mu u(a_0^*, 1) - \gamma(2\mu - 2)\mu u(a_0^*, -1)] \\



    %\frac{\partial V(\mu_\gamma(1))}{\partial \gamma}(1/2 + \gamma(2\mu - 1)) - \\
    %& \qquad\quad
    %\frac{\partial V(\mu_\gamma(0))}{\partial \gamma}(1/2 - \gamma(2\mu - 1)) \\
%\end{split}
%\]


%}}}

%	PROBLEM 4 {{{
%----------------------------------------------------------------------------------------

% }}}
\end{document}
